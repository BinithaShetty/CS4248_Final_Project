{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwGfjXD9iwhN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, SpatialDropout1D, Conv1D, MaxPooling1D, GRU, CuDNNLSTM\n",
        "from keras.layers import Bidirectional, LeakyReLU, Activation\n",
        "from keras import regularizers\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "plt.style.use(style=\"seaborn\")\n",
        "%matplotlib inline\n",
        "nltk.download('popular')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oLnb524jkH8"
      },
      "outputs": [],
      "source": [
        "train_set = pd.read_csv('raw_data/fulltrain.csv')\n",
        "train_set.columns = ['label', 'text']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#take the first 500 words from first and last paragraph each\n",
        "data= []\n",
        "for index, row in train_set.iterrows():\n",
        "    text = row['text']\n",
        "    #split into paragraphs\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    np = len(paragraphs)\n",
        "    if len(paragraphs[0].split()) < 500:\n",
        "      fist_half = paragraphs[0]\n",
        "    else:\n",
        "      fist_half = ' '.join(paragraphs[0].split()[:500])\n",
        "    if np > 1 and len(paragraphs[np-1].split()) < 500:\n",
        "      second_half = paragraphs[np-1]\n",
        "    else:\n",
        "      second_half = ' '.join(paragraphs[np-1].split()[:500])\n",
        "    new_sentence = fist_half + '. ' + second_half\n",
        "    data.append([row.label, new_sentence])\n",
        "df = pd.DataFrame(data, columns=['label', 'text'])\n",
        "train_set = df"
      ],
      "metadata": {
        "id": "dZyzB4C40sUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRIpBLtToKGG"
      },
      "source": [
        "#Define utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1fjPbZBk9jU"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    # keeps punctuation marks and question marks\n",
        "    raw = str.maketrans('', '', string.punctuation[1:20] + string.punctuation[21:])\n",
        "    return text.translate(raw)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def transform_lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "def perform_stemming(text):\n",
        "    porter = PorterStemmer()\n",
        "    text = [porter.stem(word) for word in text.split()]\n",
        "    return \" \".join(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLAuzjmsosqf"
      },
      "outputs": [],
      "source": [
        "def pad_sequences_and_truncate(sequences, max_len):\n",
        "    res = []\n",
        "    for s in sequences:\n",
        "        if len(s) >= 2 * max_len:\n",
        "            # take the first and last max_len/2 words\n",
        "            res.append(s[:max_len//2] + s[-max_len//2:])\n",
        "        elif len(s) > max_len and len(s) < 2 * max_len:\n",
        "            res.append(s[:max_len])\n",
        "        else:\n",
        "            res.append(s + [0] * (max_len - len(s)))\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXNRSfLZoP8e"
      },
      "source": [
        "#Perform preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGCURKwIl9hy"
      },
      "outputs": [],
      "source": [
        "train_set['text'] = train_set.text.map(lambda x: transform_lower(x))\n",
        "train_set['text'] = train_set.text.map(lambda x: remove_stopwords(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AByQ4fnmo3ID"
      },
      "source": [
        "#Create corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmrH4F1eo7Ao"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def create_corpus_tk(df):\n",
        "  corpus = []\n",
        "  for text in train_set['text']:\n",
        "    words = [word.lower() for word in word_tokenize(text)]\n",
        "    corpus.append(words)\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_8DSCZjpPi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f00cb48c-cc53-4d9a-b173-ac2266b355b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48853\n"
          ]
        }
      ],
      "source": [
        "corpus = create_corpus_tk(train_set)\n",
        "num_words = len(corpus)\n",
        "print(num_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GArqnVArqprB"
      },
      "outputs": [],
      "source": [
        "padding_len = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unJ9aqH5vVx7"
      },
      "source": [
        "#Implementing LSTM baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGU9Hukyosqh"
      },
      "outputs": [],
      "source": [
        "def get_lstm_model(num_words, embedding_matrix, padding_len):\n",
        "  filters = 100\n",
        "  kernel_size = 5\n",
        "  lstm_units = 32\n",
        "  embed_dim = 200\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=num_words, output_dim=embed_dim, weights=[embedding_matrix], input_length=padding_len, trainable=False))\n",
        "  model.add(SpatialDropout1D(0.5))\n",
        "  model.add(Conv1D(filters, kernel_size=kernel_size, kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  #model.add(Bidirectional(LSTM(lstm_units, dropout=0.5, recurrent_dropout=0.5,return_sequences=True)))\n",
        "  model.add(Bidirectional(CuDNNLSTM(lstm_units, return_sequences=True)))\n",
        "  model.add(SpatialDropout1D(0.5))\n",
        "  model.add(Conv1D(filters, kernel_size=kernel_size, kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  #model.add(Bidirectional(LSTM(lstm_units ,dropout=0.5, recurrent_dropout=0.5, return_sequences=True)))\n",
        "  model.add(Bidirectional(CuDNNLSTM(lstm_units, return_sequences=True)))\n",
        "  model.add(SpatialDropout1D(0.5))\n",
        "  model.add(Conv1D(filters, kernel_size=kernel_size, kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  #model.add(Bidirectional(LSTM(lstm_units, dropout=0.5, recurrent_dropout=0.5)))\n",
        "  model.add(Bidirectional(CuDNNLSTM(lstm_units)))\n",
        "  model.add(Dense(50, input_shape=(lstm_units,)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(16, input_shape=(50,)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVqxo7tDosqi"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('raw_data/balancedtest.csv')\n",
        "test.columns = ['label', 'text']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model_1"
      ],
      "metadata": {
        "id": "1hb63b9Nfq-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_1 = train_set.copy(deep = True)\n",
        "train_set_1.loc[train_set['label'] == 1, ['label']] = 1\n",
        "train_set_1.loc[train_set['label'] == 2, ['label']] = 0\n",
        "train_set_1.loc[train_set['label'] == 3, ['label']] = 0\n",
        "train_set_1.loc[train_set['label'] == 4, ['label']] = 0"
      ],
      "metadata": {
        "id": "62HCotvFVsGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZnp9PiTtDFh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#create word_embedding using glove\n",
        "embedding_dict = {}\n",
        "with open(\"glove.twitter.27B.200d.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vectors = np.asarray(values[1:], \"float32\")\n",
        "    embedding_dict[word] = vectors\n",
        "f.close()\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(train_set_1['text'])\n",
        "word_index = tokenizer.word_index\n",
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "for word, i in word_index.items():\n",
        "  if i < num_words:\n",
        "    vector = embedding_dict.get(word)\n",
        "    if vector is not None:\n",
        "      embedding_matrix[i] = vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qkvp_y5p6hg"
      },
      "outputs": [],
      "source": [
        "X = train_set_1['text'].values\n",
        "Y = pd.get_dummies(train_set_1['label']).values\n",
        "X_train, Y_train, X_train_labels, Y_train_labels = train_test_split(X, Y, test_size = 0.1, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNHiuI72r0Yd"
      },
      "outputs": [],
      "source": [
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "Y_train = tokenizer.texts_to_sequences(Y_train)\n",
        "X_train_padded_raw = pad_sequences_and_truncate(X_train, padding_len)\n",
        "Y_train_padded_raw = pad_sequences_and_truncate(Y_train, padding_len)\n",
        "\n",
        "# transform to numpy ndarray otherwise memory error\n",
        "X_train_padded = pad_sequences(X_train_padded_raw, padding='post', maxlen=padding_len, truncating='post')\n",
        "Y_train_padded = pad_sequences(Y_train_padded_raw, padding='post', maxlen=padding_len, truncating='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDnsT7Faosqh"
      },
      "outputs": [],
      "source": [
        "model_1 = get_lstm_model(num_words, embedding_matrix, padding_len)\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn2RDPHcwxPS"
      },
      "outputs": [],
      "source": [
        "history = model_1.fit(\n",
        "    X_train_padded,\n",
        "    X_train_labels,\n",
        "    epochs=20,\n",
        "    validation_split=0.1,\n",
        "    verbose=1,\n",
        "    batch_size=128,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAxXkM-Posqi"
      },
      "outputs": [],
      "source": [
        "accr = model_1.evaluate(Y_train_padded, Y_train_labels)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0], accr[1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_1 = test.copy(deep = True)\n",
        "test_1.loc[test['label'] == 1, ['label']] = 1\n",
        "test_1.loc[test['label'] == 2, ['label']] = 0\n",
        "test_1.loc[test['label'] == 3, ['label']] = 0\n",
        "test_1.loc[test['label'] == 4, ['label']] = 0"
      ],
      "metadata": {
        "id": "r_G2AmLlWfRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldu6kSLposqj"
      },
      "outputs": [],
      "source": [
        "X_test = tokenizer.texts_to_sequences(test_1['text'].values)\n",
        "X_test = pad_sequences_and_truncate(X_test, padding_len)\n",
        "X_test = pad_sequences(X_test, maxlen=padding_len, truncating='post', padding='post')\n",
        "Y_test = pd.get_dummies(test_1['label']).values\n",
        "accr = model_1.evaluate(X_test, Y_test)\n",
        "print('Test set \\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0], accr[1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_probs_1 = model_1.predict(X_test) \n",
        "y_classes_1 = y_probs_1.argmax(axis=1)\n",
        "y_probs_1_max = np.amax(y_probs_1, axis = 1)\n",
        "prediction_array_1 = np.dstack((y_classes_1, y_probs_1_max))"
      ],
      "metadata": {
        "id": "ItcvocYriIzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aN5wj7isosqj"
      },
      "outputs": [],
      "source": [
        "predict_x = model_1.predict(X_test) \n",
        "classes_x = np.argmax(predict_x, axis=1)\n",
        "actual_x = np.argmax(Y_test, axis=1)\n",
        "\n",
        "f1_score(actual_x, classes_x, average=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2"
      ],
      "metadata": {
        "id": "M3YcmUOceKq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_2 = train_set.copy(deep = True)\n",
        "train_set_2.loc[train_set['label'] == 1, ['label']] = 0\n",
        "train_set_2.loc[train_set['label'] == 2, ['label']] = 1\n",
        "train_set_2.loc[train_set['label'] == 3, ['label']] = 0\n",
        "train_set_2.loc[train_set['label'] == 4, ['label']] = 0"
      ],
      "metadata": {
        "id": "chF2TIOneQgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create word_embedding using glove\n",
        "embedding_dict = {}\n",
        "with open(\"glove.twitter.27B.200d.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vectors = np.asarray(values[1:], \"float32\")\n",
        "    embedding_dict[word] = vectors\n",
        "f.close()\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(train_set_2['text'])\n",
        "word_index = tokenizer.word_index\n",
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "for word, i in word_index.items():\n",
        "  if i < num_words:\n",
        "    vector = embedding_dict.get(word)\n",
        "    if vector is not None:\n",
        "      embedding_matrix[i] = vector"
      ],
      "metadata": {
        "id": "T2OoVSRweXFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_set_2['text'].values\n",
        "Y = pd.get_dummies(train_set_2['label']).values\n",
        "X_train, Y_train, X_train_labels, Y_train_labels = train_test_split(X, Y, test_size = 0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "b-U3tar1edKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "Y_train = tokenizer.texts_to_sequences(Y_train)\n",
        "X_train_padded_raw = pad_sequences_and_truncate(X_train, padding_len)\n",
        "Y_train_padded_raw = pad_sequences_and_truncate(Y_train, padding_len)\n",
        "\n",
        "# transform to numpy ndarray otherwise memory error\n",
        "X_train_padded = pad_sequences(X_train_padded_raw, padding='post', maxlen=padding_len, truncating='post')\n",
        "Y_train_padded = pad_sequences(Y_train_padded_raw, padding='post', maxlen=padding_len, truncating='post')"
      ],
      "metadata": {
        "id": "jYS9YdLreign"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = get_lstm_model(num_words, embedding_matrix, padding_len)\n",
        "model_2.summary()"
      ],
      "metadata": {
        "id": "TpUPLZB7elkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_2.fit(\n",
        "    X_train_padded,\n",
        "    X_train_labels,\n",
        "    epochs=20,\n",
        "    validation_split=0.1,\n",
        "    verbose=1,\n",
        "    batch_size=128,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "nSc5gcQheqcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accr = model_2.evaluate(Y_train_padded, Y_train_labels)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0], accr[1]))"
      ],
      "metadata": {
        "id": "zyx8GxOseuY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_2 = test.copy(deep = True)\n",
        "test_2.loc[test['label'] == 1, ['label']] = 0\n",
        "test_2.loc[test['label'] == 2, ['label']] = 1\n",
        "test_2.loc[test['label'] == 3, ['label']] = 0\n",
        "test_2.loc[test['label'] == 4, ['label']] = 0"
      ],
      "metadata": {
        "id": "FY3GsRj4ewpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = tokenizer.texts_to_sequences(test_2['text'].values)\n",
        "X_test = pad_sequences_and_truncate(X_test, padding_len)\n",
        "X_test = pad_sequences(X_test, maxlen=padding_len, truncating='post', padding='post')\n",
        "Y_test = pd.get_dummies(test_2['label']).values\n",
        "accr = model_2.evaluate(X_test, Y_test)\n",
        "print('Test set \\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0], accr[1]))"
      ],
      "metadata": {
        "id": "HS6Y6iOoe2im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_probs_2 = model_2.predict(X_test) \n",
        "y_classes_2 = y_probs_2.argmax(axis=1)\n",
        "y_probs_2_max = np.amax(y_probs_2, axis = 1)\n",
        "prediction_array_2 = np.dstack((y_classes_2, y_probs_2_max))"
      ],
      "metadata": {
        "id": "m2zlLZHvieFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_x = model_2.predict(X_test) \n",
        "classes_x = np.argmax(predict_x, axis=1)\n",
        "actual_x = np.argmax(Y_test, axis=1)\n",
        "\n",
        "f1_score(actual_x, classes_x, average=None)"
      ],
      "metadata": {
        "id": "gVs2eLa6fCIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3"
      ],
      "metadata": {
        "id": "GiWLy8jkgxZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_3 = train_set.copy(deep = True)\n",
        "train_set_3.loc[train_set['label'] == 1, ['label']] = 0\n",
        "train_set_3.loc[train_set['label'] == 2, ['label']] = 0\n",
        "train_set_3.loc[train_set['label'] == 3, ['label']] = 1\n",
        "train_set_3.loc[train_set['label'] == 4, ['label']] = 0\n",
        "train_set_3.groupby('label').count()"
      ],
      "metadata": {
        "id": "ZbgX1mxRhLNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create word_embedding using glove\n",
        "embedding_dict = {}\n",
        "with open(\"glove.twitter.27B.200d.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vectors = np.asarray(values[1:], \"float32\")\n",
        "    embedding_dict[word] = vectors\n",
        "f.close()\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(train_set_3['text'])\n",
        "word_index = tokenizer.word_index\n",
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "for word, i in word_index.items():\n",
        "  if i < num_words:\n",
        "    vector = embedding_dict.get(word)\n",
        "    if vector is not None:\n",
        "      embedding_matrix[i] = vector"
      ],
      "metadata": {
        "id": "hb94JVsGhQm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_set_3['text'].values\n",
        "Y = pd.get_dummies(train_set_3['label']).values\n",
        "X_train, Y_train, X_train_labels, Y_train_labels = train_test_split(X, Y, test_size = 0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "AK2NLlUchTc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "Y_train = tokenizer.texts_to_sequences(Y_train)\n",
        "X_train_padded_raw = pad_sequences_and_truncate(X_train, padding_len)\n",
        "Y_train_padded_raw = pad_sequences_and_truncate(Y_train, padding_len)\n",
        "\n",
        "# transform to numpy ndarray otherwise memory error\n",
        "X_train_padded = pad_sequences(X_train_padded_raw, padding='post', maxlen=padding_len, truncating='post')\n",
        "Y_train_padded = pad_sequences(Y_train_padded_raw, padding='post', maxlen=padding_len, truncating='post')"
      ],
      "metadata": {
        "id": "I1zqqJ7WhXcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3 = get_lstm_model(num_words, embedding_matrix, padding_len)\n",
        "model_3.summary()"
      ],
      "metadata": {
        "id": "LZElGvJEhawI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_3.fit(\n",
        "    X_train_padded,\n",
        "    X_train_labels,\n",
        "    epochs=20,\n",
        "    validation_split=0.1,\n",
        "    verbose=1,\n",
        "    batch_size=128,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "OglLF6oFhdiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accr = model_3.evaluate(Y_train_padded, Y_train_labels)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0], accr[1]))"
      ],
      "metadata": {
        "id": "u_PlJ9-VhkDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_3 = test.copy(deep = True)\n",
        "test_3.loc[test['label'] == 1, ['label']] = 0\n",
        "test_3.loc[test['label'] == 2, ['label']] = 0\n",
        "test_3.loc[test['label'] == 3, ['label']] = 1\n",
        "test_3.loc[test['label'] == 4, ['label']] = 0"
      ],
      "metadata": {
        "id": "EK8pwgJqhqWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = tokenizer.texts_to_sequences(test_3['text'].values)\n",
        "X_test = pad_sequences_and_truncate(X_test, padding_len)\n",
        "X_test = pad_sequences(X_test, maxlen=padding_len, truncating='post', padding='post')\n",
        "Y_test = pd.get_dummies(test_3['label']).values\n",
        "accr = model_3.evaluate(X_test, Y_test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0], accr[1]))"
      ],
      "metadata": {
        "id": "QWpsgMVjhvqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_probs_3 = model_3.predict(X_test) \n",
        "y_classes_3 = y_probs_3.argmax(axis=1)\n",
        "y_probs_3_max = np.amax(y_probs_3, axis = 1)\n",
        "prediction_array_3 = np.dstack((y_classes_3, y_probs_3_max))"
      ],
      "metadata": {
        "id": "wKWtwPVDkTja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_x = model_3.predict(X_test) \n",
        "classes_x = np.argmax(predict_x, axis=1)\n",
        "actual_x = np.argmax(Y_test, axis=1)\n",
        "\n",
        "f1_score(actual_x, classes_x, average=None)"
      ],
      "metadata": {
        "id": "XAz0WriTkfub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 4"
      ],
      "metadata": {
        "id": "A-ZrqIVpk_v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_4 = train_set.copy(deep = True)\n",
        "train_set_4.loc[train_set['label'] == 1, ['label']] = 0\n",
        "train_set_4.loc[train_set['label'] == 2, ['label']] = 0\n",
        "train_set_4.loc[train_set['label'] == 3, ['label']] = 0\n",
        "train_set_4.loc[train_set['label'] == 4, ['label']] = 1\n",
        "train_set_4.groupby('label').count()"
      ],
      "metadata": {
        "id": "CsjY1wLrlCrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create word_embedding using glove\n",
        "embedding_dict = {}\n",
        "with open(\"glove.twitter.27B.200d.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vectors = np.asarray(values[1:], \"float32\")\n",
        "    embedding_dict[word] = vectors\n",
        "f.close()\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(train_set_4['text'])\n",
        "word_index = tokenizer.word_index\n",
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "for word, i in word_index.items():\n",
        "  if i < num_words:\n",
        "    vector = embedding_dict.get(word)\n",
        "    if vector is not None:\n",
        "      embedding_matrix[i] = vector"
      ],
      "metadata": {
        "id": "anxT7qnqlRIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_set_4['text'].values\n",
        "Y = pd.get_dummies(train_set_4['label']).values\n",
        "X_train, Y_train, X_train_labels, Y_train_labels = train_test_split(X, Y, test_size = 0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "5Y73gp2slVug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "Y_train = tokenizer.texts_to_sequences(Y_train)\n",
        "X_train_padded_raw = pad_sequences_and_truncate(X_train, padding_len)\n",
        "Y_train_padded_raw = pad_sequences_and_truncate(Y_train, padding_len)\n",
        "\n",
        "# transform to numpy ndarray otherwise memory error\n",
        "X_train_padded = pad_sequences(X_train_padded_raw, padding='post', maxlen=padding_len, truncating='post')\n",
        "Y_train_padded = pad_sequences(Y_train_padded_raw, padding='post', maxlen=padding_len, truncating='post')"
      ],
      "metadata": {
        "id": "NvxLjpA-laBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding_matrix = get_embedding_matrix(tokenizer)\n",
        "model_4 = get_lstm_model(num_words, embedding_matrix, padding_len)\n",
        "model_4.summary()"
      ],
      "metadata": {
        "id": "xWJdpiXVldT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_4.fit(\n",
        "    X_train_padded,\n",
        "    X_train_labels,\n",
        "    epochs=20,\n",
        "    validation_split=0.1,\n",
        "    verbose=1,\n",
        "    batch_size=128,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "Sa6RGtm4lhCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accr = model_4.evaluate(Y_train_padded, Y_train_labels)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0], accr[1]))"
      ],
      "metadata": {
        "id": "2GwBl1K5lla2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_4 = test.copy(deep = True)\n",
        "test_4.loc[test['label'] == 1, ['label']] = 0\n",
        "test_4.loc[test['label'] == 2, ['label']] = 0\n",
        "test_4.loc[test['label'] == 3, ['label']] = 0\n",
        "test_4.loc[test['label'] == 4, ['label']] = 1\n",
        "test_4.groupby('label').count()"
      ],
      "metadata": {
        "id": "VibEOEP7loXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = tokenizer.texts_to_sequences(test_4['text'].values)\n",
        "X_test = pad_sequences_and_truncate(X_test, padding_len)\n",
        "X_test = pad_sequences(X_test, maxlen=padding_len, truncating='post', padding='post')\n",
        "Y_test = pd.get_dummies(test_4['label']).values\n",
        "accr = model_4.evaluate(X_test, Y_test)\n",
        "print('Test set \\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0], accr[1]))"
      ],
      "metadata": {
        "id": "w-mYzK9rluL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_probs_4 = model_4.predict(X_test) \n",
        "y_classes_4 = y_probs_4.argmax(axis=1)\n",
        "y_probs_4_max = np.amax(y_probs_4, axis = 1)\n",
        "prediction_array_4 = np.dstack((y_classes_4, y_probs_4_max))"
      ],
      "metadata": {
        "id": "8kQaJ4TXlzd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_x = model_4.predict(X_test) \n",
        "classes_x = np.argmax(predict_x, axis=1)\n",
        "actual_x = np.argmax(Y_test, axis=1)\n",
        "\n",
        "f1_score(actual_x, classes_x, average=None)"
      ],
      "metadata": {
        "id": "XFI006DIl7BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine Models"
      ],
      "metadata": {
        "id": "jtWWntifybQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_len = len(prediction_array_1[0])"
      ],
      "metadata": {
        "id": "ojsODqc9r7H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_list = []\n",
        "for i in range(pred_len):\n",
        "  p1 = prediction_array_1[0][i]\n",
        "  p2 = prediction_array_2[0][i]\n",
        "  p3 = prediction_array_3[0][i]\n",
        "  p4 = prediction_array_4[0][i]\n",
        "\n",
        "  p_val = {p1[1]: 'p1', p2[1]: 'p2', p3[1]: 'p3', p4[1]: 'p4'}\n",
        "\n",
        "  if p1[0] == 0.0 and p2[0] == 0.0 and p3[0] == 0.0 and p4[0] == 0.0:\n",
        "    max_pred = p_val.get(min(p_val))\n",
        "    if max_pred == 'p1':\n",
        "      max_class = 0\n",
        "    elif max_pred == 'p2':\n",
        "      max_class = 1\n",
        "    elif max_pred == 'p3':\n",
        "      max_class = 2\n",
        "    elif max_pred == 'p4':\n",
        "      max_class = 3\n",
        "  else:\n",
        "    max_score = -1\n",
        "    max_class = -1\n",
        "    if p1[0] == 1.0 and p1[1] > max_score:\n",
        "      max_class = 0\n",
        "      max_score = p1[1]\n",
        "    elif p2[0] == 1.0 and p2[1] > max_score:\n",
        "      max_class = 1\n",
        "      max_score = p2[1]\n",
        "    elif p3[0] == 1.0 and p3[1] > max_score:\n",
        "      max_class = 2\n",
        "      max_score = p3[1]\n",
        "    elif p4[0] == 1.0 and p4[1] > max_score:\n",
        "      max_class = 3\n",
        "      max_score = p4[1]\n",
        "  \n",
        "  predict_list.append(max_class)"
      ],
      "metadata": {
        "id": "zPmuJ-HHr-t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = tokenizer.texts_to_sequences(test['text'].values)\n",
        "X_test = pad_sequences_and_truncate(X_test, padding_len)\n",
        "X_test = pad_sequences(X_test, maxlen=padding_len, truncating='post', padding='post')\n",
        "Y_test = pd.get_dummies(test['label']).values"
      ],
      "metadata": {
        "id": "Ze7YqOtsy_E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_x = np.argmax(Y_test, axis=1)\n",
        "\n",
        "f1_score(actual_x, predict_list, average=None)"
      ],
      "metadata": {
        "id": "JdELjtrPUpwc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LSTM-one-vs-rest-classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}