{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwGfjXD9iwhN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "plt.style.use(style=\"seaborn\")\n",
        "%matplotlib inline\n",
        "# nltk.download('popular')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oLnb524jkH8"
      },
      "outputs": [],
      "source": [
        "train_set = pd.read_csv('raw_data/fulltrain.csv')\n",
        "train_set.columns = ['label', 'text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8oZUdr_kLkM"
      },
      "outputs": [],
      "source": [
        "train_set.head().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# count the number of texts under each label\n",
        "train_set.groupby('label').count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Add more reliable news training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "other_train = pd.read_csv('raw_data/other_train.csv')\n",
        "\n",
        "other_train.drop(['id', 'title', 'author'], axis=1, inplace=True)\n",
        "\n",
        "# only takes label 0\n",
        "other_train = other_train[other_train['label'] == 0]\n",
        "\n",
        "other_train_fake = other_train[other_train['label'] == 1]\n",
        "\n",
        "other_train['label'] = 4\n",
        "\n",
        "other_train_fake['label'] = 2\n",
        "\n",
        "# append first 7000 other_train to train_set\n",
        "train_set = pd.concat([train_set, other_train.head(7000)])\n",
        "train_set = pd.concat([train_set, other_train_fake])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Adding more data to hoax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hoax_train = pd.read_csv('raw_data/Fake.csv')\n",
        "\n",
        "# only takes US_News and Middle-east\n",
        "# hoax_train = hoax_train[(hoax_train['subject'] == 'US_News') | (hoax_train['subject'] == 'Middle-east') | (hoax_train['subject'] == 'News')]\n",
        "\n",
        "hoax_train.drop(['title', 'subject', 'date'], axis=1, inplace=True)\n",
        "\n",
        "# add a column of 2 to the dataframe\n",
        "hoax_train['label'] = 2\n",
        "\n",
        "# reverse the order of the dataframe\n",
        "hoax_train = hoax_train.iloc[::-1]\n",
        "\n",
        "# make id of hoax_train start from 0\n",
        "hoax_train.reset_index(inplace=True)\n",
        "\n",
        "# add first 10000 data points to train_set\n",
        "train_set = pd.concat([train_set, hoax_train])\n",
        "\n",
        "train_set.drop(['index'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set.groupby('label').count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRIpBLtToKGG"
      },
      "source": [
        "#Define utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1fjPbZBk9jU"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    # keeps punctuation marks and question marks\n",
        "    raw = str.maketrans('', '', string.punctuation[1:20] + string.punctuation[21:])\n",
        "    return text.translate(raw)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def transform_lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "def pad_sequences_and_truncate(sequence, max_len):\n",
        "    if len(sequence) > max_len:\n",
        "        return sequence[:max_len]\n",
        "    else:\n",
        "        return sequence + [0] * (max_len-len(sequence))\n",
        "\n",
        "def perform_stemming(text):\n",
        "    porter = PorterStemmer()\n",
        "    text = [porter.stem(word) for word in text.split()]\n",
        "    return \" \".join(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXNRSfLZoP8e"
      },
      "source": [
        "#Perform preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGCURKwIl9hy"
      },
      "outputs": [],
      "source": [
        "train_set['text'] = train_set.text.map(lambda x: remove_punctuations(x))\n",
        "train_set['text'] = train_set.text.map(lambda x: transform_lower(x))\n",
        "train_set['text'] = train_set.text.map(lambda x: remove_stopwords(x))\n",
        "train_set['text'] = train_set.text.map(lambda x: perform_stemming(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len_array = np.asarray(train_set['text'].str.len())\n",
        "len_dict = dict(zip(len_array, np.zeros(len(len_array))))\n",
        "for i in len_array:\n",
        "    len_dict[i] += 1\n",
        "# plot the distribution\n",
        "plt.bar(len_dict.keys(), len_dict.values())\n",
        "plt.xlim([0, 15000])\n",
        "plt.ylim([0, 50])\n",
        "plt.xlabel('Text Length')\n",
        "plt.ylabel('Number of Texts')\n",
        "plt.title('Distribution of Text Length')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Split the text into paragraphs of 100 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split the text into paragraphs of 100 words each\n",
        "\n",
        "# print(\"Initial length of train_set: \", len(train_set))\n",
        "\n",
        "# paragraphs = []\n",
        "# labels = []\n",
        "\n",
        "# for i in range(len(train_set['text'].values)):\n",
        "#     text = train_set['text'].values[i]\n",
        "#     cur = text.split()\n",
        "#     cur_label = train_set['label'].values[i]\n",
        "#     for j in range(0, len(cur), 100):\n",
        "#         if len(cur) < 100:\n",
        "#             paragraphs.append(\" \".join(cur))\n",
        "#         else:\n",
        "#             paragraphs.append(\" \".join(cur[j:j+100]))\n",
        "#         labels.append(cur_label)\n",
        "\n",
        "# train_set = pd.DataFrame({'text': paragraphs, 'label': labels})\n",
        "\n",
        "# print(\"Final length of train_set: \", len(train_set))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(\"Paragraph length: \", len(train_set['text'].values[23000].split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u-RrzQtoUoT"
      },
      "source": [
        "#Check result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slExznBdoBoz"
      },
      "outputs": [],
      "source": [
        "train_set.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUcNZ2HFoY0m"
      },
      "source": [
        "#Find max length of input data after preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CGvxnI9of95"
      },
      "outputs": [],
      "source": [
        "maxlen = -1\n",
        "avglen = 0\n",
        "sumlen = 0\n",
        "for item in train_set.text:\n",
        "  words = item.split()\n",
        "  maxlen = max(len(words), maxlen)\n",
        "  sumlen += len(words)\n",
        "avglen = sumlen/len(train_set.text)\n",
        "print(\"Maximum sentence lenth: \", maxlen)\n",
        "print(\"Average sentence lenth: \", avglen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AByQ4fnmo3ID"
      },
      "source": [
        "#Create corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmrH4F1eo7Ao"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def create_corpus_tk(df):\n",
        "  corpus = []\n",
        "  for text in train_set['text']:\n",
        "    words = [word.lower() for word in word_tokenize(text)]\n",
        "    corpus.append(words)\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_8DSCZjpPi7"
      },
      "outputs": [],
      "source": [
        "corpus = create_corpus_tk(train_set)\n",
        "num_words = len(corpus)\n",
        "print(num_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LApTbJlFpwYg"
      },
      "outputs": [],
      "source": [
        "corpus[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le6OijnDp3ux"
      },
      "source": [
        "##Train/Test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GArqnVArqprB"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padding_len = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78gv-Ffyrniu"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(train_set['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qkvp_y5p6hg"
      },
      "outputs": [],
      "source": [
        "X = train_set['text'].values\n",
        "Y = pd.get_dummies(train_set['label']).values\n",
        "X_train, Y_train, X_train_labels, Y_train_labels = train_test_split(X, Y, test_size = 0.1, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePzLGnFar0ap"
      },
      "outputs": [],
      "source": [
        "X_train = tokenizer.texts_to_sequences(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TxjwJei14xl"
      },
      "outputs": [],
      "source": [
        "Y_train = tokenizer.texts_to_sequences(Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_sequences_and_truncate(sequences, max_len):\n",
        "    res = []\n",
        "    for s in sequences:\n",
        "        if len(s) >= 2 * max_len:\n",
        "            # take the first and last max_len/2 words\n",
        "            res.append(s[:max_len//2] + s[-max_len//2:])\n",
        "        elif len(s) > max_len and len(s) < 2 * max_len:\n",
        "            res.append(s[:max_len])\n",
        "        else:\n",
        "            res.append(s + [0] * (max_len - len(s)))\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNHiuI72r0Yd"
      },
      "outputs": [],
      "source": [
        "X_train_padded_raw = pad_sequences_and_truncate(X_train, padding_len)\n",
        "Y_train_padded_raw = pad_sequences_and_truncate(Y_train, padding_len)\n",
        "\n",
        "# transform to numpy ndarray otherwise memory error\n",
        "X_train_padded = pad_sequences(X_train_padded_raw, padding='post', maxlen=padding_len, truncating='post')\n",
        "Y_train_padded = pad_sequences(Y_train_padded_raw, padding='post', maxlen=padding_len, truncating='post')\n",
        "\n",
        "# X_train_padded = pad_sequences(X_train, padding='post', maxlen=padding_len, truncating='post')\n",
        "# Y_train_padded = pad_sequences(Y_train, padding='post', maxlen=padding_len, truncating='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(X_train_padded), X_train_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsQdhVgvsPhl"
      },
      "outputs": [],
      "source": [
        "len(X_train[11]), len(X_train_padded[11])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ2V-I0Ss-dr"
      },
      "source": [
        "#Create word embedding using gloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZnp9PiTtDFh"
      },
      "outputs": [],
      "source": [
        "embedding_dict = {}\n",
        "with open(\"glove.twitter.27B.200d.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vectors = np.asarray(values[1:], \"float32\")\n",
        "    embedding_dict[word] = vectors\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIgy2qzCtfJS"
      },
      "outputs": [],
      "source": [
        "embedding_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sQ8MNKUtwWP"
      },
      "outputs": [],
      "source": [
        "word_index = tokenizer.word_index\n",
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "for word, i in word_index.items():\n",
        "  if i < num_words:\n",
        "    vector = embedding_dict.get(word)\n",
        "    if vector is not None:\n",
        "      embedding_matrix[i] = vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t97ilAlKu64v"
      },
      "outputs": [],
      "source": [
        "embedding_matrix[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = pd.read_csv('raw_data/balancedtest.csv')\n",
        "test.columns = ['label', 'text']\n",
        "test.groupby('label').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test['text'] = test.text.map(lambda x: remove_punctuations(x))\n",
        "test['text'] = test.text.map(lambda x: transform_lower(x))\n",
        "test['text'] = test.text.map(lambda x: remove_stopwords(x))\n",
        "test['text'] = test.text.map(lambda x: perform_stemming(x))\n",
        "maxlen = -1\n",
        "avglen = 0\n",
        "sumlen = 0\n",
        "for item in test.text:\n",
        "  words = item.split()\n",
        "  maxlen = max(len(words), maxlen)\n",
        "  sumlen += len(words)\n",
        "avglen = sumlen/len(test.text)\n",
        "print(\"Maximum sentence lenth: \", maxlen)\n",
        "print(\"Average sentence lenth: \", avglen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test = tokenizer.texts_to_sequences(test['text'].values)\n",
        "X_test = pad_sequences_and_truncate(X_test, padding_len)\n",
        "X_test = pad_sequences(X_test, maxlen=padding_len, truncating='post', padding='post')\n",
        "Y_test = pd.get_dummies(test['label']).values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implement Custom Early Stopping Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time test error improved.\n",
        "            verbose (bool): If True, prints a message for each test error improvement. \n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "            trace_func (function): trace print function.        \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.test_err_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.best_weights = None\n",
        "        self.trace_func = trace_func\n",
        "        \n",
        "    def __call__(self, test_err, model):\n",
        "\n",
        "        score = -test_err\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.best_weights = model.get_weights()\n",
        "            self.save_checkpoint(test_err, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            self.trace_func(f'Current Best Accuracy: {1 + self.best_score}')\n",
        "            if self.counter >= self.patience:\n",
        "                model.stop_training = True\n",
        "                print(\"Restoring model weights from the end of the best epoch.\")\n",
        "                model.set_weights(self.best_weights)\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(test_err, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, test_err, model):\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Test error decreased ({self.test_err_min:.6f} --> {test_err:.6f}). Saving model...')\n",
        "            self.best_weights = model.get_weights()\n",
        "        self.test_err_min = test_err"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import keras\n",
        "\n",
        "class EarlyStoppingCustom(keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "        test_err = 1 - self.model.evaluate(X_test, Y_test)[1]\n",
        "        early_stopping(test_err, self.model)\n",
        "        if self.model.stop_training:\n",
        "            print(\"Early stopping at epoch, \", epoch + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unJ9aqH5vVx7"
      },
      "source": [
        "#Implementing LSTM baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, SpatialDropout1D, Conv1D, MaxPooling1D, GRU, CuDNNLSTM\n",
        "from keras.layers import Bidirectional, LeakyReLU, Activation\n",
        "from keras import regularizers\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "filters = 100\n",
        "kernel_size = 5\n",
        "lstm_units = 32\n",
        "embed_dim = 200\n",
        "epochs = 100\n",
        "\n",
        "early_stopping = EarlyStopping(patience=15, verbose=True)\n",
        "\n",
        "# opt = tfa.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.001)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_words, output_dim=embed_dim, weights=[embedding_matrix], input_length=padding_len, trainable=False))\n",
        "model.add(SpatialDropout1D(0.5))\n",
        "model.add(Conv1D(filters, kernel_size=kernel_size, kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
        "model.add(LeakyReLU(alpha=0.2))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "# model.add(Bidirectional(LSTM(lstm_units, dropout=0.5, recurrent_dropout=0.5,return_sequences=True)))\n",
        "model.add(Bidirectional(CuDNNLSTM(lstm_units, return_sequences=True)))\n",
        "model.add(SpatialDropout1D(0.5))\n",
        "model.add(Conv1D(filters, kernel_size=kernel_size, kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
        "model.add(LeakyReLU(alpha=0.2))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "# model.add(Bidirectional(LSTM(lstm_units ,dropout=0.5, recurrent_dropout=0.5, return_sequences=True)))\n",
        "model.add(Bidirectional(CuDNNLSTM(lstm_units, return_sequences=True)))\n",
        "model.add(SpatialDropout1D(0.5))\n",
        "model.add(Conv1D(filters, kernel_size=kernel_size, kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
        "model.add(LeakyReLU(alpha=0.2))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "# model.add(Bidirectional(LSTM(lstm_units, dropout=0.5, recurrent_dropout=0.5)))\n",
        "model.add(Bidirectional(CuDNNLSTM(lstm_units)))\n",
        "model.add(Dense(50, input_shape=(lstm_units,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16, input_shape=(50,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Check GPU info and availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.test.is_built_with_cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn2RDPHcwxPS"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    X_train_padded,\n",
        "    X_train_labels,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.1,\n",
        "    verbose=1,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    callbacks=[EarlyStoppingCustom()]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accr = model.evaluate(X_test, Y_test)\n",
        "print('Test set without paragraphs\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0], accr[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Calculates f1-score per class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "predict_x = model.predict(X_test) \n",
        "classes_x = np.argmax(predict_x, axis=1)\n",
        "actual_x = np.argmax(Y_test, axis=1)\n",
        "\n",
        "f1_score(actual_x, classes_x, average='macro')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
